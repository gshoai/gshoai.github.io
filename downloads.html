<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Minh Hoai Nguyen's Downloads</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Prof. Minh Hoai</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="bio.html">Biography</a></div>
<div class="menu-item"><a href="pubs_all.html">Publications</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="lab.html">Lab</a></div>
<div class="menu-item"><a href="downloads.html" class="current">Download</a></div>
<div class="menu-category">Guide for students</div>
<div class="menu-item"><a href="./guideline/onboarding.html">Onboarding/Graduation</a></div>
<div class="menu-item"><a href="./guideline/meeting.html">Project&nbsp;Meeting</a></div>
<div class="menu-item"><a href="./guideline/scientific_debug.html">Scientific&nbsp;Debugging</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Minh Hoai Nguyen &ndash; Software and Datasets</h1>
</div>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script>
    $(document).ready(function() {
      var url8 = "https://api.github.com/repos/minhhoai2/MHLfuncs";
      $.getJSON(url8, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt8").text(starsCount);
      });

      var url9 = "https://api.github.com/repos/cvlab-stonybrook/EmotionNet_CVPR2020";
      $.getJSON(url9, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt9").text(starsCount);
      });

      var url10 = "https://api.github.com/repos/yangwangx/gif2video_code";
      $.getJSON(url10, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt10").text(starsCount);
      });

      var url14 = "https://api.github.com/repos/SupreethN/Hand-CNN";
      $.getJSON(url14, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt14").text(starsCount);
      });

      var url15 = "https://api.github.com/repos/cvlab-stonybrook/ContactHands";
      $.getJSON(url15, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt15").text(starsCount);
      });

      var url16 = "https://api.github.com/repos/cvlab-stonybrook/BodyHands";
      $.getJSON(url16, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt16").text(starsCount);
      });

      var url17 = "https://api.github.com/repos/cvlab-stonybrook/HandLer";
      $.getJSON(url17, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt17").text(starsCount);
      });

      var url22 = "https://api.github.com/repos/ouyangzhibo/Image_Foveation_Python";
      $.getJSON(url22, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt22").text(starsCount);
      });

      var url23 = "https://api.github.com/repos/cvlab-stonybrook/Scanpath_Prediction";
      $.getJSON(url23, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt23").text(starsCount);
      });

      var url24 = "https://api.github.com/repos/VinAIResearch/CPM";
      $.getJSON(url24, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt24").text(starsCount);
      });

      var url25 = "https://api.github.com/repos/VinAIResearch/blur-kernel-space-exploring";
      $.getJSON(url25, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt25").text(starsCount);
      });

      var url26 = "https://api.github.com/repos/VinAIResearch/MagNet";
      $.getJSON(url26, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt26").text(starsCount);
      });

      var url27 = "https://api.github.com/repos/VinAIResearch/dict-guided";
      $.getJSON(url27, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt27").text(starsCount);
      });

      var url28 = "https://api.github.com/repos/cvlab-stonybrook/Target-absent-Human-Attention";
      $.getJSON(url28, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt28").text(starsCount);
      });

      var url29 = "https://api.github.com/repos/tqvinhcs/CrossKD";
      $.getJSON(url29, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt29").text(starsCount);
      });

      var url30 = "https://api.github.com/repos/VinAIResearch/selfsup_pcd";
      $.getJSON(url30, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt30").text(starsCount);
      });

      var url31 = "https://api.github.com/repos/Viresh-R/ExemplarFreeCounting";
      $.getJSON(url31, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt31").text(starsCount);
      });

      var url32 = "https://api.github.com/repos/cvlab-stonybrook/DM-Count";
      $.getJSON(url32, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt32").text(starsCount);
      });

      var url33 = "https://api.github.com/repos/TopoXLab/TopoCount";
      $.getJSON(url33, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt33").text(starsCount);
      });

      var url34 = "https://api.github.com/repos/cvlab-stonybrook/LearningToCountEverything";
      $.getJSON(url34, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt34").text(starsCount);
      });

      var url35 = "https://api.github.com/repos/VinAIResearch/Counting-DETR";
      $.getJSON(url35, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt35").text(starsCount);
      });

      var url36 = "https://api.github.com/repos/cvlab-stonybrook/Gazeformer";
      $.getJSON(url36, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt36").text(starsCount);
      });

      var url37 = "https://api.github.com/repos/cvlab-stonybrook/scenes100";
      $.getJSON(url37, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt37").text(starsCount);
      });

      var url38 = "https://api.github.com/repos/VinAIResearch/HyperCUT";
      $.getJSON(url38, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt38").text(starsCount);
      });

      var url39 = "https://api.github.com/repos/Yifehuang97/ICACount";
      $.getJSON(url39, function(data) {
        var starsCount = data.stargazers_count;
        $("#cnt39").text(starsCount);
      });

    });
  </script>
<table>
   <tr>
      <th style="text-align: left">Description</th>
      <th>Data </th>
      <th>Model</th>
      <th>Starred</th>
   </tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/Yifehuang97/ICACount> ICACount</a>. A GUI and algorithm for interactive counting</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt39">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/HyperCUT> HyperCut and RB2V</a>. HyperCut is an algorithm for image-to-video deblurring. RB2V is a real blur2vid dataset captured by a hardware setup with two GoPro cameras and a beam splitter. RB2V contains three subsets: RB2V-Face, RB2V-Hand, and RB2V-Street.</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt38">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/scenes100> Scenses100</a>, a model and a dataset for self-supervised scence-adaptive object detection. The dataset contains 100 videos from stationary cameras, each with a duration of several hours.</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt37">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/Gazeformer> GazeFormer</a>, a method that can predict the fixations that a person makes while searching for a target (e.g., a fork) despite the unavailability of search fixations (e.g., a person looking for a fork) for model training.</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt36">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/Counting-DETR> Counting-DTR + FSCD-147</a>, a model and a dataset for few-shot counting and detection</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt35">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/LearningToCountEverything> FamNet + FSC-147</a>, a model and a dataset for few-shot object counting</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt34">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/TopoXLab/TopoCount> TopoCount</a>, a method that uses topological constraints for crowd counting and people localization</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt33">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/DM-Count> DM-Count</a>, a method for crowd counting based on Optimal Transport loss</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt32">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/Viresh-R/ExemplarFreeCounting> Exemplar-free counting</a>, a method for counting objects in images without exemplars</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt31">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/selfsup_pcd> Self-supervised Point Clound Analysis</a>, a method for point cloud analysis based on self-supervised learning with multi-view rendering</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt30">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/tqvinhcs/CrossKD> CrossKD</a>, a method for cross modality retrieval based on knowledge distillation</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt29">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/Target-absent-Human-Attention> Target-absent human attention</a>, a method for predicting gaze behavior searching for a target that is not in a scene</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt28">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/dict-guided> VinText</a>, a daset for Vietnamese scene text recognition, and an algorithmn for scene text recognition that laverages the dictionary in both training and inference time</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt27">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/MagNet> MagMet</a>, a method for progressive semantic segmentation</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt26">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/blur-kernel-space-exploring> Blur Kernel Space</a>, a method for image debluring with encoded blur kernel space</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt25">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/VinAIResearch/CPM> CPM</a>, a method and a datset for makeup transfer</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt24">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/Scanpath_Prediction> IRL Scanpath prediction</a>, a method for predicting scanpath based on inverse reinforcement learning</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt23">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/ouyangzhibo/Image_Foveation_Python> Image foveation</a>, a python library for doing image foveation</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt22">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://sites.google.com/view/cocosearch/> Coco-Search18</a>, a laboratory-quality dataset of goal-directed behavior large enough to train deep-network models. It consists of the eye gaze behavior from 10 people searching for each of 18 target-object categories in 6202 natural-scene images, yielding about 300,000 search fixations.</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://sites.google.com/view/cocosearch/coco-freeview?authuser=0> Coco-FreeView</a>, a laboratory-quality dataset of free viewing behavior. It contains the same natural images used in COCO-Search18, but labeled with 822,602 eye fixations from a free-viewing task. </td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://sites.google.com/view/mcs-dataset/home> Microwave-Clock</a>, eye gaze dataset of search behavior for two search targes: Microwave and Clocks. The dataset contains 2183 images, each is viewed by at least 30 human participants.</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=./projects/crowd_counting_gaze/index.html> Crowd Counting Gaze</a>, a dataset containing gaze behavior of 10 human participants who are tasked with counting the number of people in 30 crowd images</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/HandLer> HandLer + YoutubeHands</a>, a method and dataset for multiple hand tracking</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt17">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/BodyHands> BodyHands</a>, a method and a datset for hand detection and hand body association</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt16">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/ContactHands> ContactHands</a>, a method and a dataset for detecting hands and estimating the physical contacts of hands</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt15">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/SupreethN/Hand-CNN> Hand-CNN</a>, Keras code and model for hand detection for images in the wild</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt14">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://www3.cs.stonybrook.edu/~cvl/projects/hand_det_attention/> TV-Hand, Coco-Hand</a>, two datasets containing in-the-wild images with hand location annotation</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=http://vision.cs.stonybrook.edu/~supreeth/Working_Hands/WorkingHands.zip> WorkingHands</a>, a dataset of hands interacting with hand-held tools, seen from a overhead camera obersving a table-top working area. This contains both synthetic and real images with segmentation masks</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip> SBU shadow dataset</a></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center"></td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/yangwangx/gif2video_code> Gif2video</a>, a method to convert a gif image into a video</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt10">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/cvlab-stonybrook/EmotionNet_CVPR2020> StockEmotion</a>, a large-scale dataset of human emotion, a pre-trained model and code for extracting emotion representation vector for an image</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt9">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://github.com/minhhoai2/MHLfuncs> MHLfuncs</a>, collection of Matlab functions that are frequently used in my research, which might also be useful for some other people. The functions support various tasks, including: classification (e.g., Kernel ridge regression, kernel computation, kNN), evaluation (e.g., computing Average precision, plotting precision recall curve, plotting ROC curve), optimization (e.g., Sgd, Asgd),  k-means (e.g., parallel k-means, permuation k-means), Misc. utility functions (e.g., create latex table, progress bar, compute square distance), Video processing (frame extraction, mp4 creation, shot boundary detection, video thread linking). Version 4.1, last updated 14 Jul 2015. </td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td style="text-align: right"><span id = "cnt8">...</span>&#9733;</td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=http://www.robots.ox.ac.uk/~vgg/software/ubc/> Upper body detector</a>, Matlab code for detecting upper bodies of people in video frames. This code is based on deformable part model, also leveraing the prototypical configrations of upper bodies in TV series for better detection. Version 1.0, last updated 22 Apr 14</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=http://www.robots.ox.ac.uk/~vgg/software/discrim_subcat/> Discriminative Sub-categorization</a>, version 1.0, last updated 29 Apr 13</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=https://www3.cs.stonybrook.edu/~minhhoai/files/MMEDv2.zip> Max-Margin Early Event Detectors</a>, version 2.0, last updated 1 Mar 2016</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=./files/SegSVM_v4.tar.gz> Joint localization and classification</a>, version 4.0, last updated 8 Jul 11.</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=./files/BeyondSlideWin_v2.tar.gz> Branch-and-bound and dynamic programming code</a>, a set of efficient algorithms for optimal localization of bounding box and temporal segments, based on branch and bound and dynamic programming algorithms. Version 2.0, last updated 8 Jul 2011.</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=./files/Pre-image.zip> Robust Kernel PCA</a>, implementation of a robust kernel principal component analysis algorithm, and demo code for its applications to handle noise, occlusion, and missing data in images. Version 1.0</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
   <tr>
      <td>&#x2022; <a href=./files/SVMFeatWeight_src1.0.tar.gz> Feature selection for SVM</a>, version 1.0</td>
      <td style="text-align: center"></td>
      <td style="text-align: center">&check;</td>
      <td></td>
   </tr>
   <tr><td></td></tr>
   <tr><td></td></tr>
</table>


<div id="footer">
<div id="footer-text">
Page generated 2024-01-29 15:22:23 ACDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
