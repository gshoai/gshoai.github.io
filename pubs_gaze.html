<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Minh Hoai Nguyen Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Prof. Minh Hoai</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="bio.html">Biography</a></div>
<div class="menu-item"><a href="pubs_all.html">Publications</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="service.html">Service</a></div>
<div class="menu-item"><a href="lab.html">Lab</a></div>
<div class="menu-item"><a href="downloads.html">Download</a></div>
<div class="menu-category">Guide for students</div>
<div class="menu-item"><a href="./guideline/onboarding.html">Onboarding/Graduation</a></div>
<div class="menu-item"><a href="./guideline/meeting.html">Project&nbsp;Meeting</a></div>
<div class="menu-item"><a href="./guideline/scientific_debug.html">Scientific&nbsp;Debugging</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Minh Hoai Nguyen &ndash; Publications    </h1>
</div>
<div class="infoblock"> 
<div class="blockcontent"> 
<p>
Please cite using <font color = "#0000FF" size="4">M. Hoai</font> for publications in or after 2011 and <font color = "#0000FF" size="4">M.H. Nguyen</font> for publications before 2011.
</p>
</div></div>
<ul>
<li><p><b>All publications</b> in <a href="./pubs_all.html" target=&ldquo;blank&rdquo;>Chronological Order</a>, or in <a href="https://scholar.google.com/citations?user=hRV0tY4AAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a>
</p>
</li>
</ul>
<ul>
<li><p><b>Selected publications on main research interests</b>:
</p>
<ul>
<li><p><a href="./pubs_actReg.html" target=&ldquo;blank&rdquo;>Human Action Recognition</a>
</p>
</li>
<li><p><a href="./pubs_earlyDet.html" target=&ldquo;blank&rdquo;>Early Event Detection and Prediction</a>
</p>
</li>
<li><p><a href="./pubs_gaze.html" target=&ldquo;blank&rdquo;>Human Gaze Analysis</a>
</p>
</li>
<li><p><a href="./pubs_hand.html" target=&ldquo;blank&rdquo;>Hand Detection</a>
</p>
</li>
<li><p><a href="./pubs_count.html" target=&ldquo;blank&rdquo;>Visual Counting</a>
</p>
</li>
<li><p><a href="./pubs_weakanno.html" target=&ldquo;blank&rdquo;>Learning from weakly-, noisily-, or un-labeled data</a>
</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><b>Selected publications on other topics</b>:
</p>
<ul>
<li><p><a href="./pubs_face.html" target=&ldquo;blank&rdquo;>Facial Behavior Analysis</a>
</p>
</li>
<li><p><a href="./pubs_shadow.html" target=&ldquo;blank&rdquo;>Deep-learning for Illumination and Material Analysis</a>
</p>
</li>
</ul>

</li>
</ul>
<h2>Referred publications on Human Gaze Prediction</h2>
<table id="paper">
<tr class="r1"><td class="c1"><img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm-zerogaze.jpg" width="250px" /> </td><td class="c2"> <b>Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention</b>.<br />S. Mondal, Z. Yang, S. Ahn, D. Samaras, G. Zelinsky, M. Hoai (2023)<br /><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/Gazeformer_CVPR23.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="https://youtu.be/5ACbxDmvLZU" target=&ldquo;blank&rdquo;>Spotlight</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/Gazeformer_poster_new.pdf" target=&ldquo;blank&rdquo;>Poster</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/CVPR_Gazeformer_only_slides.pptx" target=&ldquo;blank&rdquo;>Slides</a> <a href="https://github.com/cvlab-stonybrook/Gazeformer/" target=&ldquo;blank&rdquo;>Code</a> <a href="./bibtex.html#m_Mondal-etal-CVPR23">BibTex</a><br /> </td></tr>
<tr class="r7"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm-gazefollowing-WACV23.jpg" width="250px" /> </td><td class="c2"> <b>Patch-level Gaze Distribution Prediction for Gaze Following</b>.<br />Q. Miao, M. Hoai, D. Samaras (2023)<br /><i>Winter Conference on Applications of Computer Vision (WACV)</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/GazeFollowing-WACV23.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="./bibtex.html#m_Miao-etal-WACV23">BibTex</a><br /> </td></tr>
<tr class="r13"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_targetabsence-attention.jpg" width="250px" /> </td><td class="c2"> <b>Target-absent Human Attention</b>.<br />Z. Yang, S. Mondal, S. Ahn, M. Hoai, G. Zelinsky, D. Samaras (2022)<br /><i>European Conference on Computer Vision (ECCV)</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/Target-absent-attention_ECCV22.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/targetAbsenceAtt-poster_ECCV22.pptx" target=&ldquo;blank&rdquo;>Poster</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/targetAbsenceAtt-slides_ECCV22.pptx" target=&ldquo;blank&rdquo;>Slides</a> <a href="https://github.com/cvlab-stonybrook/Target-absent-Human-Attention" target=&ldquo;blank&rdquo;>Code</a> <a href="./bibtex.html#m_Yang-etal-ECCV22">BibTex</a><br /> </td></tr>
<tr class="r19"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_targetabsence-attention2.jpg" width="250px" /> </td><td class="c2"> <b>Characterizing Target-absent Human Attention</b>.<br />Y. Chen, Z. Yang, S. Chakraborty, S. Mondal, S. Ahn, D. Samaras, M. Hoai, G. Zelinsky (2022)<br /><i>Proceedings of CVPR International Workshop on Gaze Estimation and Prediction in the Wild</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/TargetAbsentAttention-CVPRW22.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="./bibtex.html#m_Chen-etal-CVPRW22">BibTex</a><br /> </td></tr>
<tr class="r25"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_cocoSearch18.jpg" width="250px" /> </td><td class="c2"> <b>COCO-Search18 fixation dataset for predicting goal-directed attention control</b>.<br />Y. Chen, Z. Yang, S. Ahn, D. Samaras, M. Hoai, G. Zelinsky (2021)<br /><i>Scientific Reports, <b>11</b>(8776)</i>.<br /><a href="https://www.nature.com/articles/s41598-021-87715-9.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="./bibtex.html#m_Chen-etal-SR21">BibTex</a><br /> </td></tr>
<tr class="r31"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_scanpathPred.jpg" width="250px" /> </td><td class="c2"> <b>Predicting Goal-directed Human Attention Using Inverse Reinforcement Learning</b>.<br />Z. Yang, L. Huang, Y. Chen, Z. Wei, S. Ahn, G. Zelinsky, D. Samaras, M. Hoai (2020)<br /><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/scanpath-Pred_CVPR20.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/scanpathPred_CVPR20_1min.mp4" target=&ldquo;blank&rdquo;>Spotlight</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/scanpathPred_CVPR20_5min.mp4" target=&ldquo;blank&rdquo;>Oral-Presentation</a> <a href="https://sites.google.com/view/cocosearch/" target=&ldquo;blank&rdquo;>Project</a> <a href="./bibtex.html#m_Yang-etal-CVPR20">BibTex</a><br /> Nominated for Best Paper Awards (among 26 out of 5,865 submissions)</td></tr>
<tr class="r37"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_ndbt.jpg" width="250px" /> </td><td class="c2"> <b>Predicting Goal-directed Attention Control Using Inverse-Reinforcement Learning</b>.<br />G. Zelinsky, Y. Chen, S. Ahn, H. Adeli, Z. Yang, L. Huang, D. Samaras, M. Hoai (2020)<br /><i>Journal of Neurons, Behavior, Data analysis, and Theory</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/MCS_NBDT20.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="https://sites.google.com/view/mcs-dataset/home" target=&ldquo;blank&rdquo;>Data</a> <a href="./bibtex.html#m_Zelinsky-etal-NBDT20">BibTex</a><br /> </td></tr>
<tr class="r43"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/ccgaze_sm.jpg" width="250px" /> </td><td class="c2"> <b>A Study of Human Gaze Behavior During Visual Crowd Counting</b>.<br />R. Annadi, Y. Chen, V. Ranjan, D. Samaras, G. Zelinsky, M. Hoai (2020)<br /><i>arXiv:2009.06502</i>.<br /><a href="https://arxiv.org/abs/2009.06502" target=&ldquo;blank&rdquo;>Paper</a> <a href="./projects/crowd_counting_gaze/index.html" target=&ldquo;blank&rdquo;>Project</a> <a href="https://www3.cs.stonybrook.edu/~minhhoai/files/SBU_crowd_counting_gaze_data.zip" target=&ldquo;blank&rdquo;>Data</a> <a href="./bibtex.html#m_Annadi-etal-arxiv20">BibTex</a><br /> </td></tr>
<tr class="r49"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_perc18.jpg" width="250px" /> </td><td class="c2"> <b>Machine Learning Predicts Responses to Conceptual Questions Using Eye Movements</b>.<br />S. Rebello, M. Hoai, Y. Wang, T. Zu, J. Hutson, L. Loschky (2018)<br /><i>Proceedings of the Physics Education Research Conference</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/PERC2018_Rebello.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="./bibtex.html#m_Robello-PERC18">BibTex</a><br /> </td></tr>
<tr class="r55"><td class="c1">
<img src="files/mcs_dataset.jpg" width="250px" /> </td><td class="c2"> <b>Benchmarking Gaze Prediction for Categorical Visual Search</b>.<br />G. Zelinsky, Z. Yang, L. Huang, Y. Chen, S. Ahn, Z. Wei, H. Adeli, D. Samaras, M. Hoai (2019)<br /><i>CVPR Workshop - Mutual Benefits of Cognitive and Computer Vision</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/Gaze_Benchmark_CVPRW19.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="https://sites.google.com/view/mcs-dataset/home" target=&ldquo;blank&rdquo;>Data</a> <a href="./bibtex.html#m_Zelinsky-etal-MBCCV19">BibTex</a><br /> </td></tr>
<tr class="r61"><td class="c1">
<img src="https://www3.cs.stonybrook.edu/~minhhoai/files/sm_sdr.png" width="250px" /> </td><td class="c2"> <b>Learned Region Sparsity and Diversity Also Predict Visual Attention</b>.<br />Z. Wei, H. Adeli, M. Hoai, G. Zelinsky, D. Samaras (2016)<br /><i>Advances in Neural Information Processing Systems (NIPS)</i>.<br /><a href="https://www3.cs.stonybrook.edu/~minhhoai/papers/SDR_NIPS16.pdf" target=&ldquo;blank&rdquo;>Paper</a> <a href="./bibtex.html#m_Wei-etal-NIPS16">BibTex</a><br /> </td></tr>
<tr class="r67"><td class="c1">
</td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2024-01-29 21:37:49 ACDT, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
